{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopSBM: Topic Modeling with Stochastic Block Models\n",
    "\n",
    "A basic tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pylab as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sbmtm import sbmtm\n",
    "import graph_tool.all as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Load a corpus\n",
    "\n",
    "1) We have a list of documents, each document contains a list of words.\n",
    "\n",
    "2) We have a list of document titles (optional) \n",
    "\n",
    "The example corpus consists of 63 articles from Wikipedia taken from 3 different categories (Experimental Physics, Chemical Physics, and Computational Biology).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = ''\n",
    "\n",
    "## texts\n",
    "fname_data = 'corpus.txt'\n",
    "filename = os.path.join(path_data,fname_data)\n",
    "\n",
    "with open(filename,'r', encoding = 'utf8') as f:\n",
    "    x = f.readlines()\n",
    "texts = [h.split() for h in x]\n",
    "\n",
    "## titles\n",
    "fname_data = 'titles.txt'\n",
    "filename = os.path.join(path_data,fname_data)\n",
    "\n",
    "with open(filename,'r', encoding = 'utf8') as f:\n",
    "    x = f.readlines()\n",
    "titles = [h.split()[0] for h in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_doc = 0\n",
    "print(titles[0])\n",
    "print(texts[i_doc][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we create an instance of the sbmtm-class\n",
    "model = sbmtm()\n",
    "\n",
    "## we have to create the word-document network from the corpus\n",
    "model.make_graph(texts,documents=titles)\n",
    "\n",
    "## we can also skip the previous step by saving/loading a graph\n",
    "# model.save_graph(filename = 'graph.xml.gz')\n",
    "# model.load_graph(filename = 'graph.xml.gz')\n",
    "\n",
    "## fit the model\n",
    "gt.seed_rng(32) ## seed for graph-tool's random number generator --> same results\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the result\n",
    "\n",
    "The output shows the (hierarchical) community structure in the word-document network as inferred by the stochastic block model:\n",
    "\n",
    "- document-nodes are on the left\n",
    "- word-nodes are on the right\n",
    "- different colors correspond to the different groups\n",
    "\n",
    "The result is a grouping of nodes into groups on multiple levels in the hierarchy:\n",
    "\n",
    "- on the uppermost level, each node belongs to the same group (square in the middle)\n",
    "- on the next-lower level, we split the network into two groups: the word-nodes and the document-nodes (blue sqaures to the left and right, respectively). This is a trivial structure due to the bipartite character of the network.\n",
    "- only next lower levels constitute a non-trivial structure: We now further divide nodes into smaller groups (document-nodes into document-groups on the left and word-nodes into word-groups on the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot(nedges=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "For each word-group on a given level in the hierarchy, we retrieve the $n$ most common words in each group -- these are the topics!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.topics(l=1,n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-distribution in each document\n",
    "Which topics contribute to each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select a document (by its index)\n",
    "i_doc = 0\n",
    "print(model.documents[i_doc])\n",
    "## get a list of tuples (topic-index, probability)\n",
    "model.topicdist(i_doc,l=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Clustering of documents - for free.\n",
    "The stochastic block models clusters the documents into groups.\n",
    "We do not need to run an additional clustering to obtain this grouping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clusters(l=1,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application -- Finding similar articles:\n",
    "\n",
    "For a query-article, we return all articles from the same group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select a document (index)\n",
    "i_doc = 2\n",
    "print(i_doc,model.documents[i_doc])\n",
    "## find all articles from the same group\n",
    "## print: (doc-index, doc-title)\n",
    "model.clusters_query(i_doc,l=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More technical: Group membership\n",
    "In the stochastic block model, word (-nodes) and document (-nodes) are clustered into different groups.\n",
    "\n",
    "The group membership can be represented by the conditional probability $P(\\text{group}\\, |\\, \\text{node})$. Since words and documents belong to different groups (the word-document network is bipartite) we can show separately:\n",
    "\n",
    "- P(bd | d), the probability of document $d$ to belong to document group $bd$\n",
    "- P(bw | w), the probability of word $w$ to belong to word group $bw$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_td_d,p_tw_w = model.group_membership(l=1)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(p_td_d,origin='lower',aspect='auto',interpolation='none')\n",
    "plt.title(r'Document group membership $P(bd | d)$')\n",
    "plt.xlabel('Document d (index)')\n",
    "plt.ylabel('Document group, bd')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(p_tw_w,origin='lower',aspect='auto',interpolation='none')\n",
    "plt.title(r'Word group membership $P(bw | w)$')\n",
    "plt.xlabel('Word w (index)')\n",
    "plt.ylabel('Word group, bw')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
