{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. AI EDA\n",
    "\n",
    "This notebook explores the results of the AI detection analysis including:\n",
    "\n",
    "* How many AI papers we identified\n",
    "* Evolution of activity in all AI and AI categories distinguishing between papers with a category and papers labelled in a category after keyword expansion of salient terms\n",
    "* % of activities accounted by papers published at various times\n",
    "* Distribution of papers over categories and overlaps between categories in AI papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "from itertools import chain\n",
    "from narrowing_ai_research.utils.nlp import *\n",
    "from narrowing_ai_research.utils.list_utils import *\n",
    "from narrowing_ai_research.s1_paper.ai_eda import *\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "import altair as alt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want to save charts\n",
    "# driv = altair_visualisation_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{project_dir}/paper_config.yaml\",'r') as infile:\n",
    "    params = yaml.safe_load(infile)['section_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx,ai_indices,term_counts,arxiv_cat_lookup,cat_sets,cats,ai_cats = load_process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: How many papers in total\n",
    "\n",
    "ai_expanded = set(chain(*[x for x in ai_indices.values()]))\n",
    "ai_core_dupes = list(chain(*[v for k, v in cat_sets.items() if k in ai_cats]))\n",
    "ai_core = set(chain(*[v for k, v in cat_sets.items() if k in ai_cats]))\n",
    "ai_new_expanded = ai_expanded - ai_core\n",
    "ai_joint = ai_core.union(ai_expanded)\n",
    "\n",
    "results[\"ai_expanded_n\"] = len(ai_expanded)\n",
    "results[\"ai_core_with_duplicates_n\"] = len(ai_core_dupes)\n",
    "results[\"ai_core_n\"] = len(ai_core)\n",
    "results[\"ai_new_expanded_n\"] = len(ai_expanded - ai_core)\n",
    "results[\"ai_joint\"] = len(ai_joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the first trends chart and a paper trends df\n",
    "chart_1_trends,trends_df = make_agg_trend(arx,save=False)\n",
    "\n",
    "chart_1_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts shares of all AI papers at different moments in time\n",
    "\n",
    "paper_shares = make_cumulative_results(trends_df,params['years'])\n",
    "\n",
    "for rid,r in paper_shares.iterrows():\n",
    "    results[f'Share of papers published before {str(rid.date())}']=100*np.round(r['AI'],2)\n",
    "\n",
    "paper_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timecharts,catcharts = make_category_distr_time(ai_indices,arx,cats,cat_sets,arxiv_cat_lookup,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cat_trend(timecharts,save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_3 = make_cat_distr_chart(cat_sets,ai_joint,arxiv_cat_lookup,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_tokenised = {k:v for k,v in arxiv_tokenised.items() if k in ai_joint}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{project_dir}/data/interim/ai_tokenised.json\",'w') as outfile:\n",
    "    json.dump(ai_tokenised,outfile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
